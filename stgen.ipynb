{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75e3f7bc-e541-4462-908d-bf0770f2e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import json\n",
    "from datetime import datetime\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed0d36c0-ddd5-48e4-a7d2-37b2327ff90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inicializando o tokenizer e o modelo pré-treinado GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5353c923-aa1c-46aa-948c-ccbdef3abf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt e resposta salvos em stgen_output.json\n",
      "Prompt: Sun is\n",
      "Resposta: The sun is a star Sun is the star of the universe Sun has a diameter of about 1.5 billion light years. The sun has an average mass of 1,000 times that of Earth. It is about the size of a small moon.\n",
      "\n",
      "The Sun's mass is 1 billion times the mass that Earth is. Its mass has been measured at about 2.4 billion tons. This is equivalent to about one-tenth of one percent of all the Earth's total mass. In\n"
     ]
    }
   ],
   "source": [
    "# Função para gerar uma resposta\n",
    "def generate_response(prompt, max_length=100):\n",
    "    \"\"\"\n",
    "    Gera uma resposta baseada no prompt fornecido.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): O texto de entrada.\n",
    "        max_length (int): O número máximo de tokens na resposta.\n",
    "\n",
    "    Returns:\n",
    "        str: A resposta gerada.\n",
    "    \"\"\"\n",
    "    # Tokenizando o prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Gerando a resposta com o modelo\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "    # Decodificando e retornando a resposta gerada\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Função para salvar prompt e resposta em um arquivo JSON\n",
    "def save_prompt_and_response(prompt,context, response, file_path=\"stgen_output.json\"):\n",
    "    log = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"context\": context,\n",
    "        \"prompt\": prompt,\n",
    "        \"metrics\": {\n",
    "            \"coherence\": 0.85  # Exemplo de métrica\n",
    "        },\n",
    "        \"response\": response\n",
    "    }\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(log, f, indent=4)\n",
    "    print(f\"Prompt e resposta salvos em {file_path}\")\n",
    "    \n",
    "    \n",
    "# Texto estático (contexto)\n",
    "context = \"The sun is a star\"\n",
    "\n",
    "# Prompt fornecido pelo especialista\n",
    "prompt = \"Sun is\"\n",
    "\n",
    "# Gerar a resposta usando o modelo\n",
    "response = generate_response(f\"{context} {prompt}\")\n",
    "\n",
    "# Salvar a saída\n",
    "save_prompt_and_response(prompt,context, response)\n",
    "\n",
    "# Mostrar a resposta gerada\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Resposta:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d7b46-af14-4e3a-96ff-9882ea0fcf18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
